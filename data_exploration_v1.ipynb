{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A notebook for data exploration and function testing for predict sales web app "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "1. Data exploration\n",
    "2. Feature engineering and train,test and predict split\n",
    "3. Creating a base estimator to compare with\n",
    "4. Creating and testing linear model\n",
    "5. Creating and testing XGBoost model \n",
    "6. Testing on unseen shops\n",
    "7. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A long cell with functions from app/process_data.py - you can ignore it and scroll past "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_data(sales_filepath, items_filepath, use_shop_ids=[38,42,35,23,32,24,4,5,12,29]):\n",
    "    '''\n",
    "        preprocess_data() - function that transforms initial training data CSV files into a dataframe \n",
    "                            in a format that can be then processed and used in training pipeline\n",
    "        Input:\n",
    "            sales_filepath - (str) path to training sales data \n",
    "            items_filepath - (str) path to training data on items (names, categories)\n",
    "            use_shop_ids - (list) a subset of int shop ids from 'sales_filepath' CSV file to use during training to reduce training time\n",
    "        Output:\n",
    "            data - (pd.DataFrame) a Pandas dataframe with sales data\n",
    "\n",
    "    '''\n",
    "    sales_train = pd.read_csv(sales_filepath)\n",
    "    items = pd.read_csv(items_filepath)\n",
    "    data = sales_train[sales_train['shop_id'].isin(use_shop_ids) ].drop(columns=['date_block_num']).reset_index(drop=True)\n",
    "    data = pd.merge(data, items, on=['item_id'], how='left')    \n",
    "\n",
    "    return data\n",
    "\n",
    "def check_data_correctnes(data):\n",
    "    '''\n",
    "        check_data_correctnes() - function that checks correctness of a dataframe format and data, and assigns a shop_id if none is given\n",
    "                                  in case of a single shop in the data\n",
    "        Input:\n",
    "            data - (pd.DataFrame) a Pandas dataframe with sales data\n",
    "            Needed columns are: date, shop_id, item_id, item_price, item_cnt_day, item_name, item_category_id\n",
    "        Output:\n",
    "            data - (pd.DataFrame) a Pandas dataframe with sales data with necessary changes if needed (add shop_id if empty),\n",
    "            OR\n",
    "            raises an exception to let the web app know that format is incorrect \n",
    "    '''\n",
    "    data['shop_id'] = data['shop_id'].fillna('shop1')\n",
    "    return data\n",
    "\n",
    "def clean_and_aggreagate(data):\n",
    "    '''\n",
    "        clean_and_aggreagate() - function that cleanes, aggregates and sorts a Pandas dataframe for training or predicting sales\n",
    "        Input:\n",
    "            data - (pd.DataFrame) a Pandas dataframe with sales data\n",
    "        Output:\n",
    "            data_monthly - (pd.DataFrame) a cleaned and aggregated sorted dataframe\n",
    "    '''\n",
    "\n",
    "    # delete data with negative item counts or prices\n",
    "    data = data[data['item_cnt_day']>0]\n",
    "    data = data[data['item_price']>0]\n",
    "    # clean from any null values\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # convert date from string to datetime format\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%d.%m.%Y')\n",
    "\n",
    "    # Add month and year and aggregate data\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['year'] = data['date'].dt.year\n",
    "\n",
    "    # making sure we are getting only needed columns\n",
    "    keep_columns_agg = ['shop_id', 'item_id', 'item_price', 'item_cnt_day', 'month', 'year', 'item_category_id']\n",
    "    \n",
    "    # aggregate data monthly and by shops and items \n",
    "    data_monthly = data[keep_columns_agg].groupby(['year','month', 'shop_id', 'item_category_id', 'item_id'], as_index=False).agg(\n",
    "        {'item_price':'mean', \n",
    "        'item_cnt_day':['sum', 'mean'] })\n",
    "\n",
    "    # make multilevel index flat\n",
    "    data_monthly.columns = data_monthly.columns.map(''.join)\n",
    "\n",
    "    # rename columns\n",
    "    data_monthly = data_monthly.rename(columns={\n",
    "        'item_pricemean': 'item_price_avg', \n",
    "        'item_cnt_daysum':'item_cnt_month',\n",
    "        'item_cnt_daymean':'item_cnt_day_mean'})\n",
    "    \n",
    "    # sort the dataframe for future analysis\n",
    "    data_monthly = data_monthly.sort_values(by=['year', 'month', 'shop_id', 'item_id']).reset_index(drop=True)\n",
    "\n",
    "    return data_monthly\n",
    "\n",
    "def add_empty_rows(data_monthly):\n",
    "    '''\n",
    "        add_empty_rows() - function that extends a dataframe of aggregated sales with skipped rows\n",
    "                           so that the dataframe has explicit data on zero sales (sets 'item_cnt_month' to zero) \n",
    "                           instead of not mentioning an item for a particular month in a particular shop\n",
    "        Input:\n",
    "            data_monthly - (pd.DataFrame) sorted Pandas dataframe with aggregated sales data\n",
    "        Output:\n",
    "            data_monthly_ext - (pd.DataFrame) extended dataframe with explicit 0 rows\n",
    "    '''\n",
    "    item_ids = data_monthly['item_id'].unique()\n",
    "    shop_ids = data_monthly['shop_id'].unique()\n",
    "\n",
    "    start_date_year = int(data_monthly.iloc[0,:]['year'])\n",
    "    start_date_month = int(data_monthly.iloc[0,:]['month'])\n",
    "    end_date_year = int(data_monthly.iloc[-1,:]['year'])\n",
    "    end_date_month = int(data_monthly.iloc[-1,:]['month'])\n",
    "\n",
    "    # --- calculate total number of months in the period of historical data\n",
    "    number_of_months = (end_date_year - start_date_year) * 12 + (end_date_month - start_date_month) + 1\n",
    "\n",
    "    # will have data on all possible combinations of sales records\n",
    "    # for given shops, items and given time period \n",
    "    zero_rows = []\n",
    "    cur_month = start_date_month\n",
    "    cur_year = start_date_year\n",
    "\n",
    "    for i in range(number_of_months):\n",
    "        for shop in shop_ids:\n",
    "            for item in item_ids:\n",
    "                # adding a row - transaction for certain shop, year, month and item that will be zeroes initially\n",
    "                # and then joined with main data so that main data has explicit zeros as transactions\n",
    "                row = [cur_year, cur_month, shop, item]\n",
    "                zero_rows.append(row)\n",
    "        \n",
    "        # create a correction coeeficient for the case of December, 12th month, so that we het 12 and not 0\n",
    "        add_12_if_receive_0 = (12 - 12*math.ceil((cur_month+1)%12 / 12))\n",
    "        # increase month by 1, make sure it will be in [1,12] range\n",
    "        cur_month = (cur_month+1)%12 + add_12_if_receive_0\n",
    "        \n",
    "        # increase year if we arrived to the 1st month, January, after increasing month\n",
    "        if cur_month == 1:\n",
    "            cur_year += 1\n",
    "\n",
    "    # create a df of zero rows    \n",
    "    zero_rows = pd.DataFrame(zero_rows, columns=['year', 'month', 'shop_id', 'item_id'])\n",
    "\n",
    "    # merge zero rows with the main data, data_monthly\n",
    "    data_monthly_ext = pd.merge(zero_rows, data_monthly, on=['year','month', 'shop_id','item_id'], how='left')\n",
    "    # fill missing records with 0s\n",
    "    data_monthly_ext.fillna(0, inplace=True)\n",
    "    # make sure the dataframe is sorted\n",
    "    data_monthly_ext = data_monthly_ext.sort_values(by=['year', 'month', 'shop_id', 'item_id']).reset_index(drop=True)\n",
    "\n",
    "    return data_monthly_ext\n",
    "\n",
    "def add_global_features(data_monthly_ext):\n",
    "    '''\n",
    "        add_global_features() - function that adds features for training and predicting purposes.\n",
    "                                I called features global becasue they can be applied to the whole data\n",
    "                                before splitting into train and test without any data leakage in terms of forecasting\n",
    "        Input:\n",
    "            data_monthly_ext - (pd.DataFrame) a sorted by year and month Pandas dataframe with aggregated sales data\n",
    "        Output:\n",
    "            data_monthly_ext - (pd.DataFrame) initial dataframe with new features\n",
    "    '''\n",
    "    # --- add date_block to split data into train and test\n",
    "    start_month = data_monthly_ext.iloc[0,:]['month']\n",
    "    start_year = data_monthly_ext.iloc[0,:]['year']\n",
    "\n",
    "    # order of the starting month - we will subtract it from other month to understand how many month we moved forward\n",
    "    starting_month_agg = start_year*12 + start_month\n",
    "    # create arrays of year and month to perform calculations on them using np.vectorize\n",
    "    years = np.array(data_monthly_ext['year'])\n",
    "    months = np.array(data_monthly_ext['month'])\n",
    "    # for better performance create a vectorized function that computes block number (order of month from starting month)\n",
    "    calculate_block_num = np.vectorize(lambda year, month: (year*12 + month)- starting_month_agg)\n",
    "    # number of months since year 0 in starting date - this will be subtracted to understand how many months we have moved ahead from start\n",
    "    data_monthly_ext['date_block_num'] = calculate_block_num(years, months)\n",
    "\n",
    "    # Feature engineering\n",
    "    # add rolling statistics\n",
    "\n",
    "    # define size of a rolling window\n",
    "    rolling_window_size = 3\n",
    "    # --- Calculate rolling statistics\n",
    "    # keep functions in a list to iterate\n",
    "    functions = [lambda column: column.rolling(window=rolling_window_size, min_periods=1).mean(), \n",
    "                lambda column: column.rolling(window=rolling_window_size, min_periods=1).max(), \n",
    "                lambda column: column.rolling(window=rolling_window_size, min_periods=1).min(), \n",
    "                lambda column: column.rolling(window=rolling_window_size, min_periods=1).std()]\n",
    "    # these are suffixes to add to column names to generate new names\n",
    "    suffixes = ['mean', 'max', 'min', 'std']\n",
    "\n",
    "    # create len(functions) new features\n",
    "    for suff, func in zip(suffixes, functions):\n",
    "        data_monthly_ext['item_cnt_roll_{}'.format(suff)] = data_monthly_ext.groupby(['shop_id','item_id'])['item_cnt_month'].apply(func)\n",
    "\n",
    "    # Fill the empty std features with 0\n",
    "    data_monthly_ext['item_cnt_roll_std'] = data_monthly_ext['item_cnt_roll_std'].fillna(0)\n",
    "\n",
    "    # store average number of items sold per month for each item in a shop up to this point using pd.expanding function\n",
    "    data_monthly_ext['item_mean_past'] = data_monthly_ext.groupby(['shop_id', 'item_category_id'])[['item_cnt_month']].expanding().mean().values\n",
    "\n",
    "    # store average number of items sold per month for each category in a shop up to this point using pd.expanding function\n",
    "    data_monthly_ext['category_mean_past'] = data_monthly_ext.groupby(['shop_id', 'item_id'])[['item_cnt_month']].expanding().mean().values\n",
    "\n",
    "\n",
    "    # let us calculate lags for the past 3 months\n",
    "    number_lags = range(1,4)\n",
    "\n",
    "    # --- generate shifted number of items sold from the past 1-3 months\n",
    "    for lag in number_lags:\n",
    "        feature_name = 'item_cnt_shifted{}'.format(lag)\n",
    "        data_monthly_ext[feature_name] = data_monthly_ext.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt_month'].shift(lag)\n",
    "        # Fill the empty shifted features with 0\n",
    "        data_monthly_ext[feature_name] = data_monthly_ext[feature_name].fillna(0)\n",
    "\n",
    "    # generate trend which shows the change in item sales count\n",
    "    # trend = current - (previous_1 + ... + previous_n)/n = n*current - previous_1 - ... - previous_n, in our case n=3\n",
    "    # initially fill with current sales multiplied by number of times we will substract previous values to get average\n",
    "    data_monthly_ext['item_trend'] = len(number_lags) * data_monthly_ext['item_cnt_month']\n",
    "    # then subtract previous n values\n",
    "    for lag in number_lags:\n",
    "        feature_name = 'item_cnt_shifted{}'.format(lag)\n",
    "        data_monthly_ext['item_trend'] -= data_monthly_ext[feature_name]\n",
    "    # then divide by the number of times we have subtracted previous values to get average\n",
    "    data_monthly_ext['item_trend'] /= len(number_lags) \n",
    "\n",
    "    return data_monthly_ext\n",
    "\n",
    "\n",
    "def add_labels(data_monthly_ext):\n",
    "    '''\n",
    "        add_labels() - function that creates labels = a column of values we will predict = sales next month for an item in a shop\n",
    "        Input:\n",
    "            data_monthly_ext - (pd.DataFrame) a Pandas dataframe with aggregated sorted sales data\n",
    "        Output:\n",
    "            data_monthly_ext - (pd.DataFrame) dataframe with new column - 'itm_cnt_nxt_mnth', which is sales next month for an item in a shop\n",
    "    '''\n",
    "    data_monthly_ext['itm_cnt_nxt_mnth'] = data_monthly_ext.groupby(['shop_id', 'item_id'])['item_cnt_month'].shift(-1)\n",
    "    return data_monthly_ext\n",
    "\n",
    "def split_train_test_predict(data_monthly_ext):\n",
    "    '''\n",
    "        split_train_test_predict() - function that splits data into train, test and predict, where predict - is the last available month in the data,\n",
    "                       and for which we will be generating prediction for the next, unknown to us month. Test will be used to see how good is the model.\n",
    "                       Since we are dealing with timeseries, we cannot simply randomly split data. We also want to split exactly by month periods, so the standard \n",
    "                       split function will not work.\n",
    "        Input:\n",
    "            data_monthly - (pd.DataFrame) a Pandas dataframe with aggregated sales data\n",
    "        Output:\n",
    "            train_set - (pd.DataFrame) training data\n",
    "            test_set - (pd.DataFrame) testing data to see how we performed\n",
    "            predict_set - (pd.DataFrame) set of sales for the last month for which we will predict future sales for the next unseen month\n",
    "    '''\n",
    "    # calculate date_block_num for splitting\n",
    "    num_months = data_monthly_ext['date_block_num'].max() + 1\n",
    "    # starting from the 3rd month as we have rolling statistics with 3 month window (date_block_num starts with 0)\n",
    "    train_low = 3 - 1\n",
    "    # get approximately 70% of the data\n",
    "    train_high = int(0.7 * num_months) - 1\n",
    "    # testing data will have all the rest of the month up to a last one, which is used for predictiong as it does not have data on next month sales\n",
    "    test_high = num_months - 1\n",
    "\n",
    "    train_set = data_monthly_ext[(data_monthly_ext['date_block_num'] >= train_low) & (data_monthly_ext['date_block_num'] <= train_high )].copy()\n",
    "    test_set = data_monthly_ext[(data_monthly_ext['date_block_num'] > train_high) & (data_monthly_ext['date_block_num'] < test_high )].copy()\n",
    "    predict_set = data_monthly_ext[data_monthly_ext['date_block_num'] == test_high].copy()\n",
    "\n",
    "    # in train and test get rid of the null values in target variable\n",
    "    train_set = train_set.dropna(subset=['itm_cnt_nxt_mnth'])\n",
    "    test_set = test_set.dropna(subset=['itm_cnt_nxt_mnth'])\n",
    "\n",
    "    return train_set, test_set, predict_set\n",
    "\n",
    "\n",
    "\n",
    "def add_set_features(train, test):\n",
    "    '''\n",
    "        add_set_features() - function for more feature engineering done on each train and test sets to avoid data leakage.\n",
    "                            We will be calculating statistics based on the sales in the next month, so cannot use it on predict set as\n",
    "                            it does not have information on the future sales. On test set we are setting values calculated based on train data\n",
    "                            because we want to avoid data leakage and test data \"knowing\" about sales we want to predict.\n",
    "        Input:\n",
    "            train_set - (pd.DataFrame) training data\n",
    "            test_set - (pd.DataFrame) testing data to see how we performed\n",
    "        Output:\n",
    "            train_set - (pd.DataFrame) training data with new features\n",
    "            test_set - (pd.DataFrame) testing data with new features based on training data without data that can help forecast values on test,\n",
    "                        only past values from train are added here\n",
    "    '''\n",
    "    # each new feature will be a statistics computed by grouping and aggregating next month sales by certain dimensions\n",
    "    # we will generate that grouped statistics and then merge larger ungrouped train and test dataframes with it\n",
    "    # computing on train data for both train and test as we don't want to give away any information to test set\n",
    "    # as the calculations are done on future sale and since we are dealing with timeseries\n",
    "\n",
    "    def generate_statistics(dataset, group_by_columns, new_column_names, agg_column='itm_cnt_nxt_mnth', agg_function_names=['mean']):\n",
    "        '''\n",
    "            generate_statistics() - helper internal function for generating statistics of a dataset using grouping and aggregation\n",
    "            Input:\n",
    "                dataset - (pd.DataFrame) data frame to calculate statistics on\n",
    "                group_by_columns - (list of str) columns to group by\n",
    "                new_column_names - (list of str) how to name new resulting columns\n",
    "                agg_column - (str) on which column the calculation will be performed\n",
    "                agg_function_names - (list of str) which aggregate functions to use on 'agg_column' after grouping\n",
    "\n",
    "            Output:\n",
    "                res - (pd.DataFrame) dataframe with 'new_column_names' columns - result of grouping and applying aggreagate functions\n",
    "        '''\n",
    "        res = dataset.groupby(group_by_columns).agg({agg_column: agg_function_names})\n",
    "        res.columns = new_column_names\n",
    "        res.reset_index(inplace=True)\n",
    "        return res\n",
    "\n",
    "\n",
    "    # --- Add mean statistic features to train and test sets.\n",
    "\n",
    "    # Averages for each item across all shops, months and years\n",
    "    item_means = generate_statistics(train, ['item_id'], ['item_mean_future'])\n",
    "    train = pd.merge(train, item_means, on=['item_id'], how='left')\n",
    "    test = pd.merge(test, item_means, on=['item_id'], how='left')\n",
    "\n",
    "    # Averages for each item in each shop across all months and years\n",
    "    shop_item_means = generate_statistics(train, ['shop_id', 'item_id'], ['shop_item_mean_future'])\n",
    "    train = pd.merge(train, shop_item_means, on=['shop_id', 'item_id'], how='left')\n",
    "    test = pd.merge(test, shop_item_means, on=['shop_id', 'item_id'], how='left')\n",
    "\n",
    "    # Averages for each shop across all years, months and items\n",
    "    shop_means = generate_statistics(train, ['shop_id'], ['shop_mean_future'])\n",
    "    train = pd.merge(train, shop_means, on=['shop_id'], how='left')\n",
    "    test = pd.merge(test, shop_means, on=['shop_id'], how='left')\n",
    "\n",
    "    # Averages for each category across all shops, months and years\n",
    "    category_means = generate_statistics(train, ['item_category_id'], ['category_mean_future'])\n",
    "    train = pd.merge(train, category_means, on=['item_category_id'], how='left')\n",
    "    test = pd.merge(test, category_means, on=['item_category_id'], how='left')\n",
    "\n",
    "    # Averages for each year across all shops, months and items\n",
    "    year_means = generate_statistics(train, ['year'], ['year_mean_future'])\n",
    "    train = pd.merge(train, year_means, on=['year'], how='left')\n",
    "    test = pd.merge(test, year_means, on=['year'], how='left')\n",
    "\n",
    "    # Averages for each month across all shops, years and items\n",
    "    month_means = generate_statistics(train, ['month'], ['month_mean_future'])\n",
    "    train = pd.merge(train, month_means, on=['month'], how='left')\n",
    "    test = pd.merge(test, month_means, on=['month'], how='left')    \n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_data_labels(train_set, test_set, predict_set):\n",
    "    '''\n",
    "        split_data_labels() - function that splits sets into data (features) and labels to predict, \n",
    "            specifically train and test into X_train, Y_train, X_test, Y_test \n",
    "            and also creates X_predict for prediction in the same format as X_train and X_test.\n",
    "            X_predict - is the last available month in the data, and for which we will be generating prediction for the next, unknown to us, month. \n",
    "            Test will be used to see how good is the model.\n",
    "            Since we are dealing with timeseries, we cannot simply randomly split data. We also want to split exactly by month periods, so the standard \n",
    "            split function will not work.\n",
    "        Input:\n",
    "            train_set - (pd.DataFrame) training data\n",
    "            test_set - (pd.DataFrame) testing data to see how we performed\n",
    "            predict_set - (pd.DataFrame) set of sales for the last month for which we will predict future sales for the next unseen month\n",
    "        Output:\n",
    "            X_train - (pd.DataFrame) training features\n",
    "            Y_train - (pd.DataFrame) training labels\n",
    "            X_test - (pd.DataFrame) testing features\n",
    "            Y_test - (pd.DataFrame) testing labels\n",
    "            X_predict - (pd.DataFrame) features for predicting unknown data  \n",
    "    '''\n",
    "    # create train and test sets and labels. \n",
    "    X_train = train_set.drop(['itm_cnt_nxt_mnth', 'date_block_num'], axis=1)\n",
    "    Y_train = train_set['itm_cnt_nxt_mnth'].astype(int)\n",
    "    X_test = test_set.drop(['itm_cnt_nxt_mnth', 'date_block_num'], axis=1)\n",
    "    Y_test = test_set['itm_cnt_nxt_mnth'].astype(int)\n",
    "    \n",
    "    # create X_predict to predct next unseen month\n",
    "    history = pd.concat([train_set, test_set]).drop_duplicates(subset=['item_id'], keep='last')\n",
    "    X_predict = pd.merge(predict_set, history, on=['item_id'], how='left', suffixes=['', '_'])\n",
    "    X_predict.drop('itm_cnt_nxt_mnth', axis=1, inplace=True)\n",
    "    X_predict = X_predict[X_train.columns]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, X_predict\n",
    "    \n",
    "\n",
    "def return_processed_data(data):\n",
    "    '''\n",
    "        return_processed_data() - function that combines all of the ETL steps for ML training and predicting.\n",
    "                                  Raises exception if datafile is not in needed format or has less than 6 month of data\n",
    "        Input:\n",
    "            data - (pd.DataFrame) dataframe with sales data.\n",
    "                    Needed columns are: date, shop_id, item_id, item_price, item_cnt_day, item_name, item_category_id\n",
    "        Output:\n",
    "            X_train - (pd.DataFrame) training features\n",
    "            Y_train - (pd.DataFrame) training labels\n",
    "            X_test - (pd.DataFrame) testing features\n",
    "            Y_test - (pd.DataFrame) testing labels\n",
    "            X_predict - (pd.DataFrame) data set (features) for predicting unknown data\n",
    "            extended_predict_set - (pd.DataFrame) data set for predicting data, like X_predict, but with all the original columns (like item_id and etc.) to return to a user later\n",
    "    '''\n",
    "    data = check_data_correctnes(data)\n",
    "    data_monthly = clean_and_aggreagate(data)\n",
    "    data_monthly_ext = add_empty_rows(data_monthly)\n",
    "    data_monthly_ext = add_global_features(data_monthly_ext)\n",
    "    data_monthly_ext = add_labels(data_monthly_ext)\n",
    "    train_set, test_set, predict_set = split_train_test_predict(data_monthly_ext)\n",
    "    train_set, test_set = add_set_features(train_set, test_set)\n",
    "    X_train, Y_train, X_test, Y_test, X_predict = split_data_labels(train_set, test_set, predict_set)\n",
    "\n",
    "\n",
    "    # --- select features that will be used for training, testing and predicting\n",
    "    features = ['month_mean_future', 'year_mean_future', 'item_mean_future', 'shop_mean_future', 'shop_item_mean_future', 'category_mean_future',\n",
    "                'category_mean_past', 'item_mean_past', \n",
    "                'item_cnt_month', 'item_cnt_day_mean', \n",
    "                'item_cnt_roll_mean', 'item_cnt_roll_std', \n",
    "                'item_cnt_shifted1', 'item_cnt_shifted2', 'item_cnt_shifted3', \n",
    "                'item_trend', 'month']\n",
    "    # ---- Select subsets\n",
    "    # First, save the data we will be predicting before we will select features for modelling from it - 'extended_predict_set'. \n",
    "    # This will allow us to return prediction in a clear format to the user\n",
    "    extended_predict_set = X_predict\n",
    "    # --- subset other Xs            \n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "    X_predict = X_predict[features]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, X_predict, extended_predict_set\n",
    "\n",
    "def create_prediction_df(extended_predict_set, Y_predict, columns = ['year', 'month', 'shop_id', 'item_id', 'item_category_id', 'item_price_avg']):\n",
    "    '''\n",
    "        create_prediction_df() - a function that combines predicted target data (sales amount) with the data itself (shops, items, categories) into one Pandas dataframe\n",
    "        Input:\n",
    "            extended_predict_set - (pd.dataframe) dataframe with data (year, month, shops, items, categories, etc.)\n",
    "            Y_predict - (list or numpy array) dataframe with predicted sales count for the next month\n",
    "            columns = columns of extended_predict_set to keep\n",
    "        Output:\n",
    "            result_df - resulting df with combined data\n",
    "    '''    \n",
    "    result_df = extended_predict_set[columns].copy()\n",
    "\n",
    "    # get current month and year\n",
    "    cur_month = extended_predict_set['month'].iloc[0]\n",
    "    cur_year = extended_predict_set['year'].iloc[0]\n",
    "\n",
    "    # increase current month by 1 to output prediction\n",
    "    cur_month += 1\n",
    "    # check that we are less than 12, set to 1 if 13. '%' alone won't work as there is case of 12 that gives 0\n",
    "    if cur_month == 13:\n",
    "        cur_month = 1\n",
    "        # increae a year by one\n",
    "        cur_year += 1\n",
    "\n",
    "    result_df.loc[:,'year'] = cur_year\n",
    "    result_df.loc[:,'month'] = cur_month\n",
    "\n",
    "    # add predicted sales\n",
    "    result_df.loc[:,'predicted_number_items_sold'] =  Y_predict\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data('data/sales_train.csv', 'data/items.csv', use_shop_ids=range(46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1820364, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = clean_and_aggreagate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price_avg</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>item_cnt_day_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>221.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>347.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>221.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>51</td>\n",
       "      <td>128.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  shop_id  item_category_id  item_id  item_price_avg  \\\n",
       "0  2013      1        0                40       32           221.0   \n",
       "1  2013      1        0                37       33           347.0   \n",
       "2  2013      1        0                40       35           247.0   \n",
       "3  2013      1        0                40       43           221.0   \n",
       "4  2013      1        0                57       51           128.5   \n",
       "\n",
       "   item_cnt_month  item_cnt_day_mean  \n",
       "0             6.0                1.5  \n",
       "1             3.0                1.0  \n",
       "2             1.0                1.0  \n",
       "3             1.0                1.0  \n",
       "4             2.0                1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984523, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>2013.787353</td>\n",
       "      <td>0.781577</td>\n",
       "      <td>2013.00</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>6.101955</td>\n",
       "      <td>3.468553</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shop_id</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>21.593682</td>\n",
       "      <td>10.097488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_category_id</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>41.519846</td>\n",
       "      <td>16.401264</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>10622.662088</td>\n",
       "      <td>6244.475185</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5033.0</td>\n",
       "      <td>10395.0</td>\n",
       "      <td>16028.0</td>\n",
       "      <td>22169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_price_avg</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>808.740746</td>\n",
       "      <td>1585.617027</td>\n",
       "      <td>0.09</td>\n",
       "      <td>199.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>307980.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_month</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>2.328111</td>\n",
       "      <td>9.059973</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_cnt_day_mean</th>\n",
       "      <td>984523.0</td>\n",
       "      <td>1.081716</td>\n",
       "      <td>1.536588</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count          mean          std      min     25%  \\\n",
       "year               984523.0   2013.787353     0.781577  2013.00  2013.0   \n",
       "month              984523.0      6.101955     3.468553     1.00     3.0   \n",
       "shop_id            984523.0     21.593682    10.097488     0.00    15.0   \n",
       "item_category_id   984523.0     41.519846    16.401264     0.00    30.0   \n",
       "item_id            984523.0  10622.662088  6244.475185    18.00  5033.0   \n",
       "item_price_avg     984523.0    808.740746  1585.617027     0.09   199.0   \n",
       "item_cnt_month     984523.0      2.328111     9.059973     1.00     1.0   \n",
       "item_cnt_day_mean  984523.0      1.081716     1.536588     1.00     1.0   \n",
       "\n",
       "                       50%      75%       max  \n",
       "year                2014.0   2014.0    2015.0  \n",
       "month                  6.0      9.0      12.0  \n",
       "shop_id               25.0     29.0      39.0  \n",
       "item_category_id      40.0     55.0      83.0  \n",
       "item_id            10395.0  16028.0   22169.0  \n",
       "item_price_avg       399.0    899.0  307980.0  \n",
       "item_cnt_month         1.0      2.0    2253.0  \n",
       "item_cnt_day_mean      1.0      1.0    1000.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price_avg</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>item_cnt_day_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>984518</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>83</td>\n",
       "      <td>22088</td>\n",
       "      <td>119.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984519</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>83</td>\n",
       "      <td>22091</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984520</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>22100</td>\n",
       "      <td>629.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984521</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>22102</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984522</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>22162</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  shop_id  item_category_id  item_id  item_price_avg  \\\n",
       "984518  2015     10       39                83    22088           119.0   \n",
       "984519  2015     10       39                83    22091           179.0   \n",
       "984520  2015     10       39                42    22100           629.0   \n",
       "984521  2015     10       39                42    22102          1250.0   \n",
       "984522  2015     10       39                40    22162           349.0   \n",
       "\n",
       "        item_cnt_month  item_cnt_day_mean  \n",
       "984518             5.0           2.500000  \n",
       "984519             1.0           1.000000  \n",
       "984520             1.0           1.000000  \n",
       "984521             4.0           1.333333  \n",
       "984522             1.0           1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_agg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions is 1820364\n"
     ]
    }
   ],
   "source": [
    "numb_transactions = data.shape[0]\n",
    "print('Number of transactions is {}'.format(numb_transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shops is 40\n"
     ]
    }
   ],
   "source": [
    "numb_shops = data_agg['shop_id'].nunique()\n",
    "print('Number of shops is {}'.format(numb_shops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories is 74\n"
     ]
    }
   ],
   "source": [
    "numb_categories = data_agg['item_category_id'].nunique()\n",
    "print('Number of categories is {}'.format(numb_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items is 19111\n"
     ]
    }
   ],
   "source": [
    "numb_items = data_agg['item_id'].nunique()\n",
    "print('Number of unique items is {}'.format(numb_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering  and train, test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps below will take significant amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8b325840041b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_processed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-fdb0552df662>\u001b[0m in \u001b[0;36mreturn_processed_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mdata_monthly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_and_aggreagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mdata_monthly_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_empty_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_monthly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mdata_monthly_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_global_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_monthly_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mdata_monthly_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_monthly_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_test_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_monthly_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fdb0552df662>\u001b[0m in \u001b[0;36madd_global_features\u001b[0;34m(data_monthly_ext)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# create len(functions) new features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mdata_monthly_ext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_cnt_roll_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_monthly_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shop_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_cnt_month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Fill the empty std features with 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     @doc(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fdb0552df662>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(column)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# --- Calculate rolling statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# keep functions in a list to iterate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     functions = [lambda column: column.rolling(window=rolling_window_size, min_periods=1).mean(), \n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrolling_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrolling_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_rolling_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mSubstitution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rolling\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_window_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mwindow_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cython_func_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roll_mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     _shared_docs[\"median\"] = dedent(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, func, center, require_min_periods, floor, is_weighted, name, use_numba_cache, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mblock_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mwindow_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_window_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m_create_blocks\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_dict_of_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_to_dict_of_blocks\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m   5379\u001b[0m         return {\n\u001b[1;32m   5380\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5381\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5382\u001b[0m         }\n\u001b[1;32m   5383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5379\u001b[0m         return {\n\u001b[1;32m   5380\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5381\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5382\u001b[0m         }\n\u001b[1;32m   5383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__finalize__\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   5109\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5110\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5111\u001b[0;31m                 \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Series.name must be a hashable type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/inference.py\u001b[0m in \u001b[0;36mis_hashable\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \"\"\"\n\u001b[1;32m    324\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwill\u001b[0m \u001b[0msucceed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, X_predict, predict_extended = return_processed_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long cell of functions from app/train_model.py for training and evaluation, you can scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "        build_model() - function that creates a model to later train, test and save for predicting\n",
    "        Input:\n",
    "            None \n",
    "        Output:\n",
    "            model - a sklearn GridSearchCV object, model to be trained on existing data and predict categories for the new data\n",
    "    '''\n",
    "\n",
    "    \n",
    "    parameters = {\n",
    "                #\"max_depth\"        : [ 1, 2, 6],\n",
    "                \"min_child_weight\" : [3, 5]\n",
    "    }        \n",
    "    # using GridSearch for optimization is not a good idea for time series since we can only test on future data\n",
    "    cv_xgb = GridSearchCV(XGBRegressor(), param_grid = parameters, n_jobs=-1, cv=3)\n",
    "    model = cv_xgb\n",
    "    #model = XGBRegressor(min_child_weight=10)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    '''\n",
    "        evaluate_model() - function that evaluates an sklearn model\n",
    "        Input:\n",
    "            model - a trained sklearn model capable of  'predict' methods\n",
    "            X_test - data for testing, features\n",
    "            y_test - labels for testing, targets\n",
    "        Output:\n",
    "            MAE - (float) - mean absolute error across all shops and items montly data for period of time defined in X_test dataset\n",
    "    '''\n",
    "    # get the model prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    # make integers for meaningfull predictions\n",
    "    y_pred = y_pred.astype(int)\n",
    "    MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    validate_act_vs_pred = pd.DataFrame(zip(y_test, y_pred), columns=['actual', 'prediction'])\n",
    "    validate_non_zero = validate_act_vs_pred[(validate_act_vs_pred['actual'] != 0) | (validate_act_vs_pred['prediction'] != 0)]\n",
    "    MAE_non_zero = mean_absolute_error(validate_non_zero['actual'], validate_non_zero['prediction'])\n",
    "    return MAE, MAE_non_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a base estimator to compare with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating and testing Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linearModel = LinearRegression()\n",
    "print('Training model...')\n",
    "linearModel.fit(X_train, Y_train)\n",
    "print('Evaluating model...')\n",
    "mae, mae_non_zero = evaluate_model(linearModel, X_test, Y_test)\n",
    "print('MAE and MAE non-zero on test data are {} and {}'.format(mae, mae_non_zero) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating and testing XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "model = build_model()\n",
    "        \n",
    "print('Training model...')\n",
    "model.fit(X_train, Y_train)\n",
    "print('XGBRegressor with this parameters turned out to be the best:', model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating model...')\n",
    "mae, mae_non_zero = evaluate_model(model, X_test, Y_test)\n",
    "print('MAE and MAE non-zero on test data are {} and {}'.format(mae, mae_non_zero) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBRegressor with this parameters turned out to be the best:', model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print('Saving model...')\n",
    "file_pkl = open('models/forecast.pkl', 'wb')\n",
    "pickle.dump(model, file_pkl)\n",
    "file_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  See what features are the most important for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing on unseen shops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = preprocess_data('data/sales_train.csv', 'data/items.csv', use_shop_ids=range(46, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>123016.000000</td>\n",
       "      <td>123016.000000</td>\n",
       "      <td>123016.000000</td>\n",
       "      <td>123016.000000</td>\n",
       "      <td>123016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>46.460875</td>\n",
       "      <td>9634.519518</td>\n",
       "      <td>1005.884712</td>\n",
       "      <td>1.191934</td>\n",
       "      <td>37.876049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498469</td>\n",
       "      <td>6260.342913</td>\n",
       "      <td>1847.494646</td>\n",
       "      <td>1.406043</td>\n",
       "      <td>17.739795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>4231.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>7805.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>15100.000000</td>\n",
       "      <td>1180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>22167.000000</td>\n",
       "      <td>32990.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             shop_id        item_id     item_price   item_cnt_day  \\\n",
       "count  123016.000000  123016.000000  123016.000000  123016.000000   \n",
       "mean       46.460875    9634.519518    1005.884712       1.191934   \n",
       "std         0.498469    6260.342913    1847.494646       1.406043   \n",
       "min        46.000000      28.000000       0.100000      -2.000000   \n",
       "25%        46.000000    4231.000000     299.000000       1.000000   \n",
       "50%        46.000000    7805.000000     549.000000       1.000000   \n",
       "75%        47.000000   15100.000000    1180.000000       1.000000   \n",
       "max        47.000000   22167.000000   32990.000000     251.000000   \n",
       "\n",
       "       item_category_id  \n",
       "count     123016.000000  \n",
       "mean          37.876049  \n",
       "std           17.739795  \n",
       "min            2.000000  \n",
       "25%           23.000000  \n",
       "50%           37.000000  \n",
       "75%           55.000000  \n",
       "max           83.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, Y_train_new, X_test_new, Y_test_new, X_predict_new, predict_extended = return_processed_data(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "[02:53:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Evaluating model...\n",
      "MAE and MAE non-zero on test data are 0.29431850926904235 and 2.8632901929636034\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "print('Loading model...')\n",
    "model = joblib.load(\"models/forecast_v1.pkl\")\n",
    "\n",
    "print('Evaluating model...')\n",
    "mae, mae_non_zero = evaluate_model(model, X_test_new, Y_test_new)\n",
    "print('MAE and MAE non-zero on test data are {} and {}'.format(mae, mae_non_zero) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
